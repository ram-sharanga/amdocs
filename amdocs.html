<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Amdocs</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        margin: 20px;
      }
      h1,
      h2 {
        color: #2c3e50;
      }
      h3, .highlight {
        color: #16a085;
      }
      ul {
        list-style-type: disc;
        padding-left: 20px;
      }
      .category {
        margin-bottom: 20px;
      }
      table {
        width: 100%;
        border-collapse: collapse;
      }
      table,
      th,
      td {
        border: 1px solid black;
      }
      th,
      td {
        padding: 10px;
        text-align: left;
      }
      th {
        background-color: #f2f2f2;
      }
      caption {
        font-size: 1.5em;
        margin-bottom: 10px;
      }
      .tech-stack {
        background-color: #f7f7f7;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 20px;
      }
      pre {
        background-color: #eaeaea;
        padding: 10px;
        border-radius: 5px;
      }
    </style>
  </head>
  <body>
<h3>9 Jan</h3>
<table border="1" style="border-collapse: collapse; width: 100%; text-align: left;">
  <thead>
    <tr>
      <th>Step</th>
      <th>Details</th>
      <th>Your Approach</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1. Input</td>
      <td>Collect messages with <span class="highlight">user consent</span> in real-time.</td>
      <td>Using an app that receives user consent.</td>
    </tr>
    <tr>
      <td>2. Extraction</td>
      <td>Extract claims or key information from messages.</td>
      <td>Using <span class="highlight">GPT-Neo</span> LLM (equivalent to GPT-3) for claim extraction.</td>
    </tr>
    <tr>
      <td>3. Verification</td>
      <td>Compare extracted claims with trusted sources.</td>
      <td>Using open-source <span class="highlight">Rapid API</span> (other options include Zyla API, google gemini API) to access third-party data.</td>
    </tr>
    <tr>
      <td>4. Output</td>
      <td>Notify users of the message's status (verified/unverified).</td>
      <td>Send reports to user over whatsapp / Notifications from our app to flag messages along with reports</td>
    </tr>
  </tbody>
</table>
 <hr/>
    <h3>8 Jan</h3>
    <table>
      <caption>
        Misinformation Detection: Comparison Data Sources
      </caption>
      <thead>
        <tr>
          <th>Information Type</th>
          <th>Comparison Data Sources</th>
          <th>Explanation</th>
          <th>Solution to Tackle</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Text</td>
          <td>
            <ul>
              <li><a href="https://www.politifact.com/">PolitiFact</a></li>
              <li><a href="https://www.factcheck.org/">FactCheck.org</a></li>
              <li><a href="https://www.snopes.com/">Snopes</a></li>
              <li>
                <a href="https://www.washingtonpost.com/news/fact-checker/"
                  >Washington Post Fact Checker</a
                >
              </li>
              <li><a href="https://fullfact.org/">Full Fact</a></li>
              <li>
                <a href="https://www.ifcncodeofprinciples.poynter.org/">IFCN</a>
              </li>
            </ul>
          </td>
          <td>
            These sources provide verified and fact-checked text data that can
            be used for comparison to detect misinformation in user-submitted
            text.
          </td>
          <td>
            Use fact-checking sites to cross-verify claims, extract valid data,
            and compare with input text. Use NLP models like BERT for
            misinformation detection.
          </td>
        </tr>
        <tr>
          <td>Audio</td>
          <td>
            <ul>
              <li>
                <a href="https://www.bbc.co.uk/worldserviceradio"
                  >BBC World Service</a
                >
              </li>
              <li><a href="https://www.npr.org/">NPR</a></li>
              <li>
                <a href="https://www.reuters.com/audio/">Reuters Audio</a>
              </li>
              <li><a href="https://www.voanews.com/">Voice of America</a></li>
              <li><a href="https://www.rev.com/">Rev</a></li>
              <li><a href="https://trint.com/">Trint</a></li>
            </ul>
          </td>
          <td>
            Audio sources like news outlets and transcription services provide
            reliable content for comparison against audio claims.
          </td>
          <td>
            Use speech-to-text models like Google Speech API to transcribe
            audio. Compare transcriptions with verified audio data sources for
            verification.
          </td>
        </tr>
        <!-- <tr>
          <td>Image</td>
          <td>
            <ul>
              <li>
                <a href="https://images.google.com/"
                  >Google Reverse Image Search</a
                >
              </li>
              <li><a href="https://www.tineye.com/">TinEye</a></li>
              <li><a href="https://yandex.com/images/">Yandex</a></li>
              <li><a href="https://www.politifact.com/">PolitiFact</a></li>
              <li><a href="https://www.factcheck.org/">FactCheck.org</a></li>
              <li><a href="https://www.flickr.com/">Flickr</a></li>
            </ul>
          </td>
          <td>
            These platforms allow you to trace the origin and context of images,
            verifying whether the image is being used misleadingly.
          </td>
          <td>
            Use reverse image search engines to trace image sources and verify
            the context. Integrate image classification models to detect image
            manipulation.
          </td>
        </tr>
        <tr>
          <td>Video</td>
          <td>
            <ul>
              <li>
                <a href="https://www.bbc.com/realitycheck">BBC Reality Check</a>
              </li>
              <li><a href="https://factcheck.afp.com/">AFP Fact Check</a></li>
              <li><a href="https://www.invid-project.eu/">InVID</a></li>
              <li>
                <a href="https://www.youtube.com/yt/about/">YouTube Data API</a>
              </li>
              <li>
                <a href="https://www.amnesty.org/en/latest/research/"
                  >Amnesty International Video Verification</a
                >
              </li>
            </ul>
          </td>
          <td>
            These sources verify video content, debunking hoaxes or manipulating
            videos circulating online.
          </td>
          <td>
            Use video metadata analysis and frame-level verification. Employ
            tools like InVID for cross-referencing video content with verified
            databases.
          </td>
        </tr> -->
      </tbody>
    </table>

    <hr />

    <!-- <div>
      <h1>Misinformation Detection Approach</h1>

      <h2>1. Data Collection & Preprocessing</h2>
      <p>
        <strong>Goal:</strong> Gather diverse datasets (text, audio, image,
        video) for training and preprocessing them into usable formats.
      </p>

      <h3>Tech Stack:</h3>
      <div class="tech-stack">
        <h4>Data Collection:</h4>
        <ul>
          <li>Text: Twitter API</li>
          <li>Video: YouTube API</li>
          <li>Images: ImageNet</li>
          <li>Audio: LibriSpeech</li>
        </ul>
        <h4>Data Preprocessing:</h4>
        <ul>
          <li>
            Text: <strong>nltk</strong>, <strong>spaCy</strong> for
            tokenization, named entity recognition, and lemmatization.
          </li>
          <li>
            Audio: <strong>Librosa</strong>, <strong>pydub</strong> for sound
            extraction, feature extraction (MFCC, Mel-spectrograms).
          </li>
          <li>
            Images: <strong>OpenCV</strong>, <strong>Pillow</strong>,
            <strong>scikit-image</strong> for image resizing, normalization, and
            augmentation.
          </li>
          <li>
            Video: <strong>OpenCV</strong>, <strong>moviepy</strong>,
            <strong>ffmpeg</strong> for frame extraction and processing.
          </li>
        </ul>
      </div>

      <h2>2. Feature Engineering & Extraction</h2>
      <p>
        <strong>Goal:</strong> Extract relevant features for each type of data
        (text, audio, image, and video).
      </p>

      <h3>Tech Stack:</h3>
      <div class="tech-stack">
        <h4>Text:</h4>
        <ul>
          <li>
            <strong>Word Embeddings:</strong> Word2Vec, GloVe, or FastText to
            convert words into vector representations.
          </li>
          <li>
            <strong>Contextual Embeddings:</strong> BERT (HuggingFace
            Transformers) for sentence-level or document-level understanding.
          </li>
        </ul>
        <h4>Audio:</h4>
        <ul>
          <li>
            <strong>Feature Extraction:</strong> Use
            <strong>Librosa</strong> for extracting MFCCs, Chroma features,
            Zero-crossing rates, etc.
          </li>
        </ul>
        <h4>Images:</h4>
        <ul>
          <li>
            <strong>Convolutional Neural Networks (CNNs):</strong>
            <strong>TensorFlow</strong> or <strong>PyTorch</strong> to extract
            image features using pre-trained models like ResNet, VGG.
          </li>
          <li>
            <strong>Feature Vectorization:</strong> Convert images to feature
            vectors using CNN-based models.
          </li>
        </ul>
        <h4>Video:</h4>
        <ul>
          <li>
            <strong>Frame-based Feature Extraction:</strong> Extract frames from
            videos using <strong>OpenCV</strong> and apply CNNs for frame-level
            feature extraction.
          </li>
          <li>
            <strong>Optical Flow or 3D CNNs:</strong> For motion analysis and
            temporal features, use 3D CNNs with <strong>TensorFlow</strong> or
            <strong>PyTorch</strong>.
          </li>
        </ul>
      </div>

      <h2>3. Model Development (Misinformation Detection)</h2>
      <p>
        <strong>Goal:</strong> Train separate models to detect misinformation
        within each data type.
      </p>

      <h3>Tech Stack:</h3>
      <div class="tech-stack">
        <h4>Text Misinformation:</h4>
        <ul>
          <li>
            <strong>Model:</strong> Fine-tune a transformer-based model such as
            BERT, RoBERTa, or DistilBERT for text classification.
          </li>
          <li>
            <strong>Framework:</strong> HuggingFace Transformers, TensorFlow,
            PyTorch.
          </li>
        </ul>
        <h4>Audio Misinformation:</h4>
        <ul>
          <li>
            <strong>Model:</strong> Use RNN-based models (e.g., LSTM, GRU) or
            transformers for sequence modeling of audio features.
          </li>
          <li><strong>Framework:</strong> TensorFlow, PyTorch.</li>
        </ul>
        <h4>Image Misinformation:</h4>
        <ul>
          <li>
            <strong>Model:</strong> Use pre-trained CNN architectures like
            ResNet, InceptionV3, or EfficientNet for image-based misinformation
            detection.
          </li>
          <li><strong>Framework:</strong> TensorFlow, PyTorch.</li>
        </ul>
        <h4>Video Misinformation:</h4>
        <ul>
          <li>
            <strong>Model:</strong> Use 3D CNNs for video classification or
            combine CNNs with LSTMs for temporal information (e.g., C3D, I3D).
          </li>
          <li><strong>Framework:</strong> TensorFlow, PyTorch, Keras.</li>
        </ul>
      </div>

      <h2>4. Model Fusion/Ensemble</h2>
      <p>
        <strong>Goal:</strong> Combine results from the individual models (text,
        audio, image, video) into a unified decision system.
      </p>

      <h3>Tech Stack:</h3>
      <div class="tech-stack">
        <ul>
          <li>
            <strong>Ensemble Methods:</strong> Use techniques such as stacking,
            bagging, or boosting.
          </li>
          <li>
            <strong>XGBoost, LightGBM, CatBoost:</strong> For model ensembling.
          </li>
          <li>
            <strong>Decision Fusion:</strong> Weighted voting or probabilistic
            fusion for combining results from each model type.
          </li>
          <li>
            <strong>Framework:</strong> Scikit-learn for traditional ensemble
            models, TensorFlow, PyTorch for deep learning-based fusion.
          </li>
        </ul>
      </div>

      <h2>5. Training & Evaluation</h2>
      <p>
        <strong>Goal:</strong> Train models, evaluate their performance, and
        tune hyperparameters.
      </p>

      <h3>Tech Stack:</h3>
      <div class="tech-stack">
        <h4>Model Training:</h4>
        <ul>
          <li>For deep learning models: TensorFlow, Keras, PyTorch.</li>
          <li>
            For traditional models (SVMs, XGBoost): Scikit-learn, XGBoost.
          </li>
        </ul>
        <h4>Hyperparameter Tuning:</h4>
        <ul>
          <li>
            <strong>Grid Search/Random Search:</strong> Scikit-learn, Keras
            Tuner.
          </li>
          <li><strong>Bayesian Optimization:</strong> Optuna.</li>
        </ul>
        <h4>Evaluation:</h4>
        <ul>
          <li>
            <strong>Metrics:</strong> Accuracy, Precision, Recall, F1-score,
            ROC-AUC for classification tasks.
          </li>
          <li>
            <strong>TensorBoard:</strong> For visualizing model performance.
          </li>
          <li>
            <strong>Scikit-learn:</strong> For traditional metrics evaluation.
          </li>
        </ul>
      </div>

      <h2>6. Deployment & Monitoring</h2>
      <p>
        <strong>Goal:</strong> Deploy the model as a service and monitor its
        performance in production.
      </p>

      <h3>Tech Stack:</h3>
      <div class="tech-stack">
        <h4>Model Deployment:</h4>
        <ul>
          <li>
            Use <strong>Flask</strong>, <strong>FastAPI</strong> for lightweight
            API deployment.
          </li>
          <li>
            For scalable deployment: <strong>Docker</strong>,
            <strong>Kubernetes</strong>.
          </li>
          <li>
            Use <strong>TensorFlow Serving</strong> or
            <strong>TorchServe</strong> for serving models.
          </li>
        </ul>
        <h4>Cloud Services:</h4>
        <ul>
          <li>
            <strong>AWS, Google Cloud, or Azure</strong> for model hosting and
            scalability.
          </li>
        </ul>
        <h4>Monitoring:</h4>
        <ul>
          <li>
            Use monitoring tools like <strong>Prometheus</strong>,
            <strong>Grafana</strong> for real-time metrics and logging.
          </li>
          <li>
            <strong>Seldon</strong>, <strong>MLflow</strong> for model
            versioning, monitoring, and experimentation.
          </li>
        </ul>
      </div>

      <h2>7. Post-Deployment Improvements (Feedback Loop)</h2>
      <p>
        <strong>Goal:</strong> Continuously update the model with new data and
        retrain periodically.
      </p>

      <h3>Tech Stack:</h3>
      <div class="tech-stack">
        <ul>
          <li>
            <strong>Model Retraining:</strong> Schedule regular model updates
            using <strong>Airflow</strong> or
            <strong>Kubeflow Pipelines</strong>.
          </li>
          <li>
            <strong>Data Collection and Labeling:</strong> Use human-in-the-loop
            systems for labeling new misinformation samples and improving the
            dataset.
          </li>
        </ul>
      </div>

      <h2>End-to-End Flow Summary</h2>
      <ul>
        <li>
          Data Collection & Preprocessing: Gather and preprocess text, audio,
          image, and video data using relevant tools.
        </li>
        <li>
          Feature Extraction: Extract features for each modality (e.g., text
          embeddings, audio features, image features, video frames).
        </li>
        <li>
          Model Development: Build and train separate models for each modality
          (text classification, audio analysis, image recognition, video
          analysis).
        </li>
        <li>
          Model Fusion: Combine outputs from individual models for final
          decision-making using ensemble methods.
        </li>
        <li>
          Training & Evaluation: Train models, perform hyperparameter tuning,
          and evaluate performance using appropriate metrics.
        </li>
        <li>
          Deployment: Deploy the model in a production environment and monitor
          it for effectiveness.
        </li>
        <li>
          Continuous Improvement: Implement a feedback loop to retrain the model
          with new data and improve accuracy over time.
        </li>
      </ul>
    </div> -->

    <h1>Comparison Sources for Misinformation Detection</h1>

    <div class="category">
      <h2>Text Comparison Sources</h2>
      <h3>Fact-Checking Websites</h3>
      <ul>
        <li>
          <a href="https://www.politifact.com/">PolitiFact</a>: Fact-checks
          political statements, claims, and articles.
        </li>
        <li>
          <a href="https://www.factcheck.org/">FactCheck.org</a>: Non-partisan
          fact-checking resource for political claims and social media content.
        </li>
        <li>
          <a href="https://www.snopes.com/">Snopes</a>: A well-known
          fact-checking website that debunks rumors, myths, and misinformation.
        </li>
        <li>
          <a href="https://www.washingtonpost.com/news/fact-checker/"
            >The Washington Post Fact Checker</a
          >: Fact-checking articles focused on political statements and claims.
        </li>
        <li>
          <a href="https://fullfact.org/">Full Fact</a>: A UK-based independent
          fact-checking organization.
        </li>
        <li>
          <a href="https://www.ifcncodeofprinciples.poynter.org/">IFCN</a>:
          International Fact-Checking Network provides a list of certified
          fact-checking organizations globally.
        </li>
      </ul>
      <h3>News Websites and Reliable Sources</h3>
      <ul>
        <li>
          <a href="https://www.reuters.com/">Reuters</a>: Reputable
          international news agency.
        </li>
        <li>
          <a href="https://www.bbc.com/">BBC</a>: Trusted news and media
          organization providing fact-checked news content.
        </li>
        <li>
          <a href="https://www.nytimes.com/">The New York Times</a>: A leading
          global news provider, often citing original sources.
        </li>
        <li>
          <a href="https://www.ap.org/">Associated Press</a>: A global news
          organization that emphasizes factual reporting.
        </li>
      </ul>
      <h3>Academic Journals and Databases</h3>
      <ul>
        <li>
          <a href="https://scholar.google.com/">Google Scholar</a>: A search
          engine for scholarly articles that provide fact-checked information
          and peer-reviewed papers.
        </li>
        <li>
          <a href="https://www.jstor.org/">JSTOR</a>: A digital library for
          academic journals, books, and primary sources that can be used for
          verifying text-based claims.
        </li>
      </ul>
    </div>

    <div class="category">
      <h2>Audio Comparison Sources</h2>
      <h3>News Audio Outlets</h3>
      <ul>
        <li>
          <a href="https://www.bbc.co.uk/sounds">BBC World Service</a>: Reliable
          news broadcasts and podcasts.
        </li>
        <li>
          <a href="https://www.npr.org/">NPR</a>: National Public Radio provides
          fact-checked audio news.
        </li>
        <li>
          <a href="https://www.reuters.com/audio">Reuters Audio</a>: Audio
          content for global news.
        </li>
        <li>
          <a href="https://www.voanews.com/">Voice of America</a>: News audio
          and radio shows, with a global reach.
        </li>
      </ul>
      <h3>Speech-to-Text Services for Verification</h3>
      <ul>
        <li>
          <a href="https://www.rev.com/">Rev</a>: Provides accurate
          transcriptions of audio for comparison.
        </li>
        <li>
          <a href="https://www.trint.com/">Trint</a>: AI-based transcription
          service for comparing transcriptions of audio clips.
        </li>
      </ul>
      <h3>Fact-Checking Audio Platforms</h3>
      <ul>
        <li>
          <a href="https://www.factcheckradio.org/">Fact Check Radio</a>:
          Audio-based fact-checking content.
        </li>
      </ul>
    </div>

    <!-- <div class="category">
      <h2>Image Comparison Sources</h2>
      <h3>Reverse Image Search Engines</h3>
      <ul>
        <li>
          <a href="https://www.google.com/imghp">Google Reverse Image Search</a
          >: Helps find the original source of an image.
        </li>
        <li>
          <a href="https://tineye.com/">TinEye</a>: Reverse image search tool
          that tracks down the origin and usage of images.
        </li>
        <li>
          <a href="https://yandex.com/images/">Yandex</a>: Another image search
          engine for tracing image origins.
        </li>
      </ul>
      <h3>Fact-Checking Image Databases</h3>
      <ul>
        <li>
          <a href="https://www.politifact.com/">PolitiFact</a>: Fact-checking
          organization that often verifies images and photos.
        </li>
        <li>
          <a href="https://www.factcheck.org/">FactCheck.org</a>: Frequently
          fact-checks images in the context of political statements or social
          media.
        </li>
      </ul>
      <h3>Open Access Image Databases</h3>
      <ul>
        <li>
          <a href="https://www.flickr.com/">Flickr</a>: A collection of
          user-uploaded images where original context can be traced.
        </li>
        <li>
          <a href="https://www.pexels.com/">Pexels</a>: High-quality images with
          usage licenses that can be used for verifying or comparing image
          content.
        </li>
      </ul>
    </div>

    <div class="category">
      <h2>Video Comparison Sources</h2>
      <h3>Fact-Checking Organizations for Videos</h3>
      <ul>
        <li>
          <a href="https://www.bbc.com/realitycheck"
            >Video Verification (BBC Reality Check)</a
          >: BBC’s video fact-checking platform.
        </li>
        <li>
          <a href="https://factcheck.afp.com/">AFP Fact Check</a>: Provides
          detailed verification of videos found on social media and other
          sources.
        </li>
        <li>
          <a href="https://fullfact.org/">Full Fact</a>: UK-based organization
          that fact-checks video content in the media.
        </li>
      </ul>
      <h3>Video Reverse Search and Metadata Verification</h3>
      <ul>
        <li>
          <a href="https://www.invid-project.eu/">InVID</a>: A video
          verification tool for searching and analyzing videos, including
          metadata.
        </li>
        <li>
          <a href="https://developers.google.com/youtube/v3">YouTube Data API</a
          >: For retrieving metadata of YouTube videos, including upload date,
          title, description, and views.
        </li>
        <li>
          <a
            href="https://www.amnesty.org/en/latest/research/2020/11/how-we-fact-checked-atrocity-videos/"
            >Amnesty International's YouTube Video Verification</a
          >: Provides resources for verifying YouTube videos and tracking
          misleading content.
        </li>
      </ul>
      <h3>Fact-Checking Video Platforms</h3>
      <ul>
        <li>
          <a href="https://www.truthorfiction.com/">Truth or Fiction</a>: A site
          that fact-checks various online content, including videos.
        </li>
        <li>
          <a href="https://hoax-slayer.net/">Hoax-Slayer</a>: Investigates and
          debunks hoaxes and false videos circulating online.
        </li>
      </ul>
      <h3>News Organizations for Video Verification</h3>
      <ul>
        <li>
          <a href="https://www.bbc.com/video">BBC</a>: Regularly publishes
          fact-checked video reports, especially in breaking news.
        </li>
        <li>
          <a href="https://www.reuters.com/video">Reuters Video</a>: Provides
          original video content and factual verification.
        </li>
      </ul>
    </div> -->

    <div class="category">
      <h2>Additional General Sources for Comparison Data</h2>
      <h3>Government and Nonprofit Websites</h3>
      <ul>
        <li>
          <a href="https://en.unesco.org/">UNESCO</a>: Provides reports and data
          on misinformation and media literacy.
        </li>
        <li>
          <a
            href="https://www.factcheck.org/2020/04/fact-checking-the-fact-checkers/"
            >Government Fact-checking Initiatives</a
          >: Governments often provide official fact-checking resources on
          current claims and information.
        </li>
      </ul>
      <h3>Crowdsourced Fact-Checking</h3>
      <ul>
        <li>
          <a href="https://www.crowdtangle.com/">CrowdTangle</a>: A platform for
          tracking and analyzing social media content, often used for
          fact-checking viral posts.
        </li>
        <li>
          <a href="https://www.wikitribune.com/">WikiTribune</a>: A
          community-driven journalism site for accurate reporting and
          fact-checking.
        </li>
      </ul>
    </div>
  </body>
</html>
